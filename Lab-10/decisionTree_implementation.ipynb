{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " id3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlook_Overcast (entropy=0.940, samples=14)\n",
      "Left:\n",
      "  Leaf node, Class distribution: [0 4]\n",
      "Right:\n",
      "  Humidity (entropy=1.000, samples=10)\n",
      "  Left:\n",
      "    Outlook_Rain (entropy=0.985, samples=7)\n",
      "    Left:\n",
      "      Leaf node, Class distribution: [1 3]\n",
      "    Right:\n",
      "      Leaf node, Class distribution: [3 0]\n",
      "  Right:\n",
      "    Outlook_Rain (entropy=0.918, samples=3)\n",
      "    Left:\n",
      "      Leaf node, Class distribution: [1 0]\n",
      "    Right:\n",
      "      Leaf node, Class distribution: [0 2]\n",
      "The predicted class for the sample is: 1\n"
     ]
    }
   ],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self,feature=None,entropy=None,samples=None,value=None,left=None,right=None) -> None:\n",
    "        self.feature = feature\n",
    "        self.entropy = entropy\n",
    "        self.samples = samples\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.left is None and self.right is None\n",
    "    \n",
    "def compute_entropy(y):\n",
    "    entropy = 0\n",
    "    if len(y) > 0:\n",
    "        p1 = len(y[y == 1])/len(y)\n",
    "        if p1 not in (0,1):\n",
    "            entropy = -p1*np.log2(p1) -(1-p1)*np.log2(1-p1)\n",
    "    return entropy\n",
    "\n",
    "def split_dataset(x,node_indices,feature):\n",
    "    left_indices,right_indices = [],[]\n",
    "    for i in node_indices:\n",
    "        if x[i][feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices,right_indices\n",
    "\n",
    "def compute_information_gain(x,y,node_indices,feature):\n",
    "    left_indices,right_indices = split_dataset(x,node_indices,feature)\n",
    "    x_node,y_node = x[node_indices],y[node_indices] \n",
    "    x_left,y_left = x[left_indices],y[left_indices]\n",
    "    x_right,y_right = x[right_indices],y[right_indices]\n",
    "    node_entropy = compute_entropy(y_node)\n",
    "    left_entropy = compute_entropy(y_left)  \n",
    "    right_entropy = compute_entropy(y_right)\n",
    "    w_left = len(x_left)/len(x)\n",
    "    w_right = len(x_right)/len(x)\n",
    "    weighted_entropy = w_left*left_entropy + w_right*right_entropy\n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "def get_best_split(x,y,node_indices):\n",
    "    num_features = x.shape[1]\n",
    "    max_info_gain = -np.inf\n",
    "    best_feature = None\n",
    "    for feature in range(num_features):\n",
    "        info_gain = compute_information_gain(x,y,node_indices,feature)\n",
    "        if info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain \n",
    "            best_feature = feature\n",
    "    return best_feature\n",
    "\n",
    "def build_id3_tree(x,y,node_indices,current_depth,max_depth):\n",
    "    entropy = compute_entropy(y[node_indices])\n",
    "    value = np.bincount(y[node_indices],minlength=2)\n",
    "    node = TreeNode(entropy=entropy,samples=len(node_indices),value=value)\n",
    "    #Stop condition\n",
    "    if current_depth == max_depth or entropy == 0 or len(np.unique(y[node_indices]))==1:\n",
    "        return node\n",
    "    best_feature = get_best_split(x,y,node_indices)\n",
    "    if best_feature is None:\n",
    "        return node\n",
    "    left_indices,right_indices = split_dataset(x,node_indices,best_feature)\n",
    "    node.feature = best_feature\n",
    "    if left_indices:\n",
    "        node.left = build_id3_tree(x,y,left_indices,current_depth+1,max_depth)\n",
    "    if right_indices:\n",
    "        node.right = build_id3_tree(x,y,right_indices,current_depth+1,max_depth)\n",
    "    return node\n",
    "\n",
    "def print_tree(node, depth=0, feature_names=None):\n",
    "    # Determine the feature name based on whether a list of feature names was provided.\n",
    "    # If the node is not a leaf and feature names are provided, use the actual feature name.\n",
    "    # Otherwise, use a generic feature label.\n",
    "    if feature_names is not None and node.feature is not None:\n",
    "        feature_name = feature_names[node.feature]\n",
    "    else:\n",
    "        feature_name = f\"Feature {node.feature}\"\n",
    "\n",
    "    # Check if the current node is a leaf node.\n",
    "    if node.is_leaf():\n",
    "        # For leaf nodes, print the class distribution of the samples.\n",
    "        print(f\"{'  ' * depth}Leaf node, Class distribution: {node.value}\")\n",
    "    else:\n",
    "        # For internal nodes, print the feature name, entropy, and number of samples.\n",
    "        print(f\"{'  ' * depth}{feature_name} (entropy={node.entropy:.3f}, samples={node.samples})\")\n",
    "\n",
    "        # If there is a left child, print the left branch of the tree.\n",
    "        if node.left is not None:\n",
    "            print(f\"{'  ' * depth}Left:\")\n",
    "            print_tree(node.left, depth + 1, feature_names)\n",
    "\n",
    "        # If there is a right child, print the right branch of the tree.\n",
    "        if node.right is not None:\n",
    "            print(f\"{'  ' * depth}Right:\")\n",
    "            print_tree(node.right, depth + 1, feature_names)\n",
    "\n",
    "# Correct function name for building the tree\n",
    "root_node = build_id3_tree(x, y, list(range(len(y))), current_depth=0, max_depth=max_depth)\n",
    "\n",
    "# Print the tree\n",
    "# Assuming df is defined elsewhere and is a pandas DataFrame\n",
    "feature_names = df.drop('Decision', axis=1).columns.tolist()\n",
    "print_tree(root_node, feature_names=feature_names)\n",
    "\n",
    "\n",
    "def predict(tree, sample, feature_names):\n",
    "    \"\"\"\n",
    "    Predict the class label for a single sample based on the decision tree.\n",
    "    \n",
    "    :param tree: The root node of the decision tree.\n",
    "    :param sample: The feature vector for the sample to predict.\n",
    "    :param feature_names: The list of feature names corresponding to the indices in the sample.\n",
    "    :return: The predicted class label.\n",
    "    \"\"\"\n",
    "    # Traverse the tree until a leaf node is reached\n",
    "    while not tree.is_leaf():\n",
    "        # If the feature on which to split is in our sample, use it; otherwise, predict randomly.\n",
    "        if tree.feature is not None and feature_names[tree.feature] in sample:\n",
    "            # Determine the index of the feature to split on\n",
    "            feature_index = feature_names.index(tree.feature)\n",
    "            # If the feature of the sample is 1, go right; otherwise, go left.\n",
    "            if sample[feature_index] == 1:\n",
    "                tree = tree.right\n",
    "            else:\n",
    "                tree = tree.left\n",
    "        else:\n",
    "            # If the feature is not in our sample, we have an issue (e.g., missing feature)\n",
    "            # For simplicity, return a random class label (0 or 1).\n",
    "            return np.random.choice([0, 1])\n",
    "    \n",
    "    # Once a leaf node is reached, return the class with the highest count\n",
    "    return np.argmax(tree.value)\n",
    "\n",
    "my_sample = {\n",
    "    'Outlook_Overcast': 0,\n",
    "    'Outlook_Rain': 1,\n",
    "    'Outlook_Sunny': 0,\n",
    "    'Temp': 0,  # Assuming this is a binary feature after preprocessing\n",
    "    'Humidity': 1,  # Assuming this is a binary feature after preprocessing\n",
    "    'Wind': 0  # Assuming this is a binary feature after preprocessing\n",
    "}\n",
    "\n",
    "# Convert the sample dictionary to a list in the order of feature names used in the tree\n",
    "sample_features = [my_sample[fn] for fn in feature_names]\n",
    "\n",
    "# Use the predict function to get the prediction for the sample\n",
    "predicted_class = predict(root_node, sample_features, feature_names)\n",
    "print(f\"The predicted class for the sample is: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
