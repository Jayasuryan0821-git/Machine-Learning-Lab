{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " id3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlook_Overcast (entropy=0.940, samples=14)\n",
      "Left:\n",
      "  Leaf node, Class distribution: [0 4]\n",
      "Right:\n",
      "  Humidity (entropy=1.000, samples=10)\n",
      "  Left:\n",
      "    Outlook_Rain (entropy=0.811, samples=4)\n",
      "    Left:\n",
      "      Leaf node, Class distribution: [0 1]\n",
      "    Right:\n",
      "      Leaf node, Class distribution: [3 0]\n",
      "  Right:\n",
      "    Wind (entropy=0.918, samples=6)\n",
      "    Left:\n",
      "      Temp (entropy=0.918, samples=3)\n",
      "      Left:\n",
      "        Leaf node, Class distribution: [0 1]\n",
      "      Right:\n",
      "        Leaf node, Class distribution: [2 0]\n",
      "    Right:\n",
      "      Leaf node, Class distribution: [0 3]\n",
      "The predicted class for the sample is: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df = pd.read_csv('data.csv')\n",
    "df['Decision'] = df['Decision'].map({'No':0,'Yes':1})\n",
    "df['Wind'] =  df['Wind'].map({'Weak':0,'Strong':1})\n",
    "df = pd.get_dummies(df,columns=['Outlook'])\n",
    "\n",
    "def convertHum(val):\n",
    "    if val > df['Humidity'].mean():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "def convertTemp(val):\n",
    "    if val > df['Temp'].mean():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "df['Humidity'] = df['Humidity'].apply(convertHum)\n",
    "df['Temp'] = df['Temp'].apply(convertTemp)\n",
    "x = df.drop('Decision',axis=1).values\n",
    "y = df['Decision'].values\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self,feature=None,entropy=None,samples=None,value=None,left=None,right=None) -> None:\n",
    "        self.feature = feature\n",
    "        self.entropy = entropy\n",
    "        self.samples = samples\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.left is None and self.right is None\n",
    "    \n",
    "def compute_entropy(y):\n",
    "    entropy = 0\n",
    "    if len(y) > 0:\n",
    "        p1 = len(y[y == 1])/len(y)\n",
    "        if p1 not in (0,1):\n",
    "            entropy = -p1*np.log2(p1) -(1-p1)*np.log2(1-p1)\n",
    "    return entropy\n",
    "\n",
    "def split_dataset(x,node_indices,feature):\n",
    "    left_indices,right_indices = [],[]\n",
    "    for i in node_indices:\n",
    "        if x[i][feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices,right_indices\n",
    "\n",
    "def compute_information_gain(x,y,node_indices,feature):\n",
    "    left_indices,right_indices = split_dataset(x,node_indices,feature)\n",
    "    x_node,y_node = x[node_indices],y[node_indices] \n",
    "    x_left,y_left = x[left_indices],y[left_indices]\n",
    "    x_right,y_right = x[right_indices],y[right_indices]\n",
    "    node_entropy = compute_entropy(y_node)\n",
    "    left_entropy = compute_entropy(y_left)  \n",
    "    right_entropy = compute_entropy(y_right)\n",
    "    w_left = len(x_left)/len(x)\n",
    "    w_right = len(x_right)/len(x)\n",
    "    weighted_entropy = w_left*left_entropy + w_right*right_entropy\n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "def get_best_split(x,y,node_indices):\n",
    "    num_features = x.shape[1]\n",
    "    max_info_gain = -np.inf\n",
    "    best_feature = None\n",
    "    for feature in range(num_features):\n",
    "        info_gain = compute_information_gain(x,y,node_indices,feature)\n",
    "        if info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain \n",
    "            best_feature = feature\n",
    "    return best_feature\n",
    "\n",
    "def build_id3_tree(x,y,node_indices,current_depth,max_depth):\n",
    "    entropy = compute_entropy(y[node_indices])\n",
    "    value = np.bincount(y[node_indices],minlength=2)\n",
    "    node = TreeNode(entropy=entropy,samples=len(node_indices),value=value)\n",
    "    #Stop condition\n",
    "    if current_depth == max_depth or entropy == 0 or len(np.unique(y[node_indices]))==1:\n",
    "        return node\n",
    "    best_feature = get_best_split(x,y,node_indices)\n",
    "    if best_feature is None:\n",
    "        return node\n",
    "    left_indices,right_indices = split_dataset(x,node_indices,best_feature)\n",
    "    node.feature = best_feature\n",
    "    if left_indices:\n",
    "        node.left = build_id3_tree(x,y,left_indices,current_depth+1,max_depth)\n",
    "    if right_indices:\n",
    "        node.right = build_id3_tree(x,y,right_indices,current_depth+1,max_depth)\n",
    "    return node\n",
    "\n",
    "def print_tree(node, depth=0, feature_names=None):\n",
    "    # Determine the feature name based on whether a list of feature names was provided.\n",
    "    # If the node is not a leaf and feature names are provided, use the actual feature name.\n",
    "    # Otherwise, use a generic feature label.\n",
    "    if feature_names is not None and node.feature is not None:\n",
    "        feature_name = feature_names[node.feature]\n",
    "    else:\n",
    "        feature_name = f\"Feature {node.feature}\"\n",
    "\n",
    "    # Check if the current node is a leaf node.\n",
    "    if node.is_leaf():\n",
    "        # For leaf nodes, print the class distribution of the samples.\n",
    "        print(f\"{'  ' * depth}Leaf node, Class distribution: {node.value}\")\n",
    "    else:\n",
    "        # For internal nodes, print the feature name, entropy, and number of samples.\n",
    "        print(f\"{'  ' * depth}{feature_name} (entropy={node.entropy:.3f}, samples={node.samples})\")\n",
    "\n",
    "        # If there is a left child, print the left branch of the tree.\n",
    "        if node.left is not None:\n",
    "            print(f\"{'  ' * depth}Left:\")\n",
    "            print_tree(node.left, depth + 1, feature_names)\n",
    "\n",
    "        # If there is a right child, print the right branch of the tree.\n",
    "        if node.right is not None:\n",
    "            print(f\"{'  ' * depth}Right:\")\n",
    "            print_tree(node.right, depth + 1, feature_names)\n",
    "\n",
    "# Correct function name for building the tree\n",
    "root_node = build_id3_tree(x, y, list(range(len(y))), current_depth=0, max_depth=6)\n",
    "\n",
    "# Print the tree\n",
    "# Assuming df is defined elsewhere and is a pandas DataFrame\n",
    "feature_names = df.drop('Decision', axis=1).columns.tolist()\n",
    "print_tree(root_node, feature_names=feature_names)\n",
    "\n",
    "\n",
    "def predict(tree, sample, feature_names):\n",
    "    \"\"\"\n",
    "    Predict the class label for a single sample based on the decision tree.\n",
    "    \n",
    "    :param tree: The root node of the decision tree.\n",
    "    :param sample: The feature vector for the sample to predict.\n",
    "    :param feature_names: The list of feature names corresponding to the indices in the sample.\n",
    "    :return: The predicted class label.\n",
    "    \"\"\"\n",
    "    # Traverse the tree until a leaf node is reached\n",
    "    while not tree.is_leaf():\n",
    "        # If the feature on which to split is in our sample, use it; otherwise, predict randomly.\n",
    "        if tree.feature is not None and feature_names[tree.feature] in sample:\n",
    "            # Determine the index of the feature to split on\n",
    "            feature_index = feature_names.index(tree.feature)\n",
    "            # If the feature of the sample is 1, go right; otherwise, go left.\n",
    "            if sample[feature_index] == 1:\n",
    "                tree = tree.right\n",
    "            else:\n",
    "                tree = tree.left\n",
    "        else:\n",
    "            # If the feature is not in our sample, we have an issue (e.g., missing feature)\n",
    "            # For simplicity, return a random class label (0 or 1).\n",
    "            return np.random.choice([0, 1])\n",
    "    \n",
    "    # Once a leaf node is reached, return the class with the highest count\n",
    "    return np.argmax(tree.value)\n",
    "\n",
    "my_sample = {\n",
    "    'Outlook_Overcast': 0,\n",
    "    'Outlook_Rain': 1,\n",
    "    'Outlook_Sunny': 0,\n",
    "    'Temp': 0,  # Assuming this is a binary feature after preprocessing\n",
    "    'Humidity': 1,  # Assuming this is a binary feature after preprocessing\n",
    "    'Wind': 0  # Assuming this is a binary feature after preprocessing\n",
    "}\n",
    "\n",
    "# Convert the sample dictionary to a list in the order of feature names used in the tree\n",
    "sample_features = [my_sample[fn] for fn in feature_names]\n",
    "\n",
    "# Use the predict function to get the prediction for the sample\n",
    "predicted_class = predict(root_node, sample_features, feature_names)\n",
    "print(f\"The predicted class for the sample is: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class for the instance is: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df['Decision'] = df['Decision'].map({'No':0,'Yes':1})\n",
    "df['Wind'] =  df['Wind'].map({'Weak':0,'Strong':1})\n",
    "df = pd.get_dummies(df,columns=['Outlook'])\n",
    "\n",
    "def convertHum(val):\n",
    "    if val > df['Humidity'].mean():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "def convertTemp(val):\n",
    "    if val > df['Temp'].mean():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "df['Humidity'] = df['Humidity'].apply(convertHum)\n",
    "df['Temp'] = df['Temp'].apply(convertTemp)\n",
    "x = df.drop('Decision',axis=1).values\n",
    "y = df['Decision'].values\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, value=None, gini=None, samples=None, left=None, right=None):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.gini = gini\n",
    "        self.samples = samples\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.left is None and self.right is None\n",
    "\n",
    "def compute_gini(y):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    gini = 1 - np.sum(probabilities ** 2)\n",
    "    return gini\n",
    "\n",
    "def split_dataset(x, node_indices, feature):\n",
    "    left_indices = [i for i in node_indices if x[i, feature] == 0]\n",
    "    right_indices = [i for i in node_indices if x[i, feature] == 1]\n",
    "    return left_indices, right_indices\n",
    "\n",
    "def compute_gini_gain(x, y, node_indices, feature):\n",
    "    left_indices, right_indices = split_dataset(x, node_indices, feature)\n",
    "    gini_node = compute_gini(y[node_indices])\n",
    "    gini_left = compute_gini(y[left_indices])\n",
    "    gini_right = compute_gini(y[right_indices])\n",
    "    weight_left = len(left_indices) / len(node_indices)\n",
    "    weight_right = len(right_indices) / len(node_indices)\n",
    "    weighted_gini = weight_left * gini_left + weight_right * gini_right\n",
    "    gini_gain = gini_node - weighted_gini\n",
    "    return gini_gain\n",
    "\n",
    "def get_best_split_cart(x, y, node_indices):\n",
    "    best_feature = None\n",
    "    best_gini_gain = -np.inf\n",
    "    for feature in range(x.shape[1]):\n",
    "        gini_gain = compute_gini_gain(x, y, node_indices, feature)\n",
    "        if gini_gain > best_gini_gain:\n",
    "            best_gini_gain = gini_gain\n",
    "            best_feature = feature\n",
    "    return best_feature, best_gini_gain\n",
    "\n",
    "def build_cart_tree(x, y, node_indices, current_depth, max_depth):\n",
    "    # Calculate the Gini impurity for the current node\n",
    "    gini = compute_gini(y[node_indices])\n",
    "    value = np.bincount(y[node_indices], minlength=2)\n",
    "    node = TreeNode(gini=gini, samples=len(node_indices), value=value)\n",
    "    \n",
    "    # Check for max depth, pure node, or no more features to split on\n",
    "    if current_depth == max_depth or gini == 0 or len(set(y[node_indices])) == 1:\n",
    "        return node\n",
    "    \n",
    "    best_feature, best_gini_gain = get_best_split_cart(x, y, node_indices)\n",
    "    if best_feature is None or best_gini_gain <= 0:\n",
    "        return node\n",
    "    \n",
    "    left_indices, right_indices = split_dataset(x, node_indices, best_feature)\n",
    "    node.feature = best_feature\n",
    "    if left_indices:\n",
    "        node.left = build_cart_tree(x, y, left_indices, current_depth + 1, max_depth)\n",
    "    if right_indices:\n",
    "        node.right = build_cart_tree(x, y, right_indices, current_depth + 1, max_depth)\n",
    "    \n",
    "    return node\n",
    "\n",
    "# Assuming you have defined your x, y, and feature_names as before\n",
    "root_node = build_cart_tree(x, y, list(range(len(y))), current_depth=0, max_depth=3)\n",
    "\n",
    "\n",
    "def print_tree(node, depth=0, feature_names=None):\n",
    "    indent = \"  \" * depth\n",
    "    if node.is_leaf():\n",
    "        print(f\"{indent}Leaf node, class distribution: {node.value}\")\n",
    "    else:\n",
    "        feature_name = feature_names[node.feature] if feature_names else f\"Feature {node.feature}\"\n",
    "        print(f\"{indent}{feature_name} (Gini={node.gini:.3f}, samples={node.samples})\")\n",
    "        if node.left is not None:\n",
    "            print(f\"{indent}Left:\")\n",
    "            print_tree(node.left, depth + 1, feature_names)\n",
    "        if node.right is not None:\n",
    "            print(f\"{indent}Right:\")\n",
    "            print_tree(node.right, depth + 1, feature_names)\n",
    "\n",
    "def predict_sample(node, sample):\n",
    "    while not node.is_leaf():\n",
    "        if sample[node.feature] == 0:\n",
    "            node = node.left\n",
    "        else:\n",
    "            node = node.right\n",
    "    return np.argmax(node.value)\n",
    "\n",
    "def predict(tree, X):\n",
    "    predictions = [predict_sample(tree, sample) for sample in X]\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Define your instance as a list or a NumPy array (ensure the order of features matches the training data)\n",
    "my_instance = [0, 1, 0, 0, 1, 0, 1, 0]  # Example instance\n",
    "\n",
    "# Convert the instance to a NumPy array if it's not already one\n",
    "my_instance = np.array(my_instance)\n",
    "\n",
    "# Predict the class for your instance using the tree\n",
    "prediction = predict_sample(root_node, my_instance)\n",
    "print(f\"The predicted class for the instance is: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class for the instance is: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, value=None, info_gain=None, samples=None, left=None, right=None):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.info_gain = info_gain\n",
    "        self.samples = samples\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.left is None and self.right is None\n",
    "\n",
    "def compute_entropy(y):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    return -np.sum(probabilities * np.log2(probabilities + 1e-9))  # Add a small epsilon to avoid log(0)\n",
    "\n",
    "def split_dataset(x, y, node_indices, feature):\n",
    "    left_indices = [i for i in node_indices if x[i, feature] == 0]\n",
    "    right_indices = [i for i in node_indices if x[i, feature] == 1]\n",
    "    return left_indices, right_indices\n",
    "\n",
    "def compute_gain_ratio(x, y, node_indices, feature):\n",
    "    # Calculate information gain\n",
    "    left_indices, right_indices = split_dataset(x, y, node_indices, feature)\n",
    "    entropy_before = compute_entropy(y[node_indices])\n",
    "    entropy_after = (\n",
    "        len(left_indices) / len(node_indices) * compute_entropy(y[left_indices]) +\n",
    "        len(right_indices) / len(node_indices) * compute_entropy(y[right_indices])\n",
    "    )\n",
    "    info_gain = entropy_before - entropy_after\n",
    "\n",
    "    # Calculate split information\n",
    "    split_info = 0\n",
    "    for indices in (left_indices, right_indices):\n",
    "        proportion = len(indices) / len(node_indices)\n",
    "        split_info -= proportion * np.log2(proportion + 1e-9)  # Add a small epsilon to avoid log(0)\n",
    "\n",
    "    # Calculate gain ratio\n",
    "    gain_ratio = info_gain / split_info if split_info != 0 else 0\n",
    "    return gain_ratio\n",
    "\n",
    "def get_best_split_c45(x, y, node_indices):\n",
    "    best_feature = None\n",
    "    best_gain_ratio = -np.inf\n",
    "    for feature in range(x.shape[1]):\n",
    "        gain_ratio = compute_gain_ratio(x, y, node_indices, feature)\n",
    "        if gain_ratio > best_gain_ratio:\n",
    "            best_gain_ratio = gain_ratio\n",
    "            best_feature = feature\n",
    "    return best_feature, best_gain_ratio\n",
    "\n",
    "def build_c45_tree(x, y, node_indices, current_depth, max_depth):\n",
    "    entropy = compute_entropy(y[node_indices])\n",
    "    value = np.bincount(y[node_indices], minlength=2)\n",
    "    node = TreeNode(info_gain=entropy, samples=len(node_indices), value=value)\n",
    "    \n",
    "    if current_depth == max_depth or entropy == 0 or len(np.unique(y[node_indices])) == 1:\n",
    "        return node\n",
    "    \n",
    "    best_feature, best_gain_ratio = get_best_split_c45(x, y, node_indices)\n",
    "    if best_feature is None or best_gain_ratio <= 0:\n",
    "        return node\n",
    "    \n",
    "    left_indices, right_indices = split_dataset(x, y, node_indices, best_feature)\n",
    "    node.feature = best_feature\n",
    "    if left_indices:\n",
    "        node.left = build_c45_tree(x, y, left_indices, current_depth + 1, max_depth)\n",
    "    if right_indices:\n",
    "        node.right = build_c45_tree(x, y, right_indices, current_depth + 1, max_depth)\n",
    "    \n",
    "    return node\n",
    "\n",
    "# Assuming you have defined your x, y, and feature_names as before\n",
    "root_node_c45 = build_c45_tree(x, y, list(range(len(y))), current_depth=0, max_depth=3)\n",
    "\n",
    "def print_tree(node, depth=0, feature_names=None):\n",
    "    # Determine the feature name based on whether a list of feature names was provided.\n",
    "    # If the node is not a leaf and feature names are provided, use the actual feature name.\n",
    "    # Otherwise, use a generic feature label.\n",
    "    feature_name = feature_names[node.feature] if (feature_names is not None and node.feature is not None) else \"N/A\"\n",
    "\n",
    "    # Check if the current node is a leaf node.\n",
    "    if node.is_leaf():\n",
    "        # For leaf nodes, print the class distribution of the samples.\n",
    "        print(f\"{'  ' * depth}Leaf node, Class distribution: {node.value}\")\n",
    "    else:\n",
    "        # For internal nodes, print the feature name, gain ratio, and number of samples.\n",
    "        print(f\"{'  ' * depth}{feature_name} (info_gain={node.info_gain:.3f}, samples={node.samples})\")\n",
    "\n",
    "        # If there is a left child, print the left branch of the tree.\n",
    "        if node.left is not None:\n",
    "            print(f\"{'  ' * depth}Left:\")\n",
    "            print_tree(node.left, depth + 1, feature_names)\n",
    "\n",
    "        # If there is a right child, print the right branch of the tree.\n",
    "        if node.right is not None:\n",
    "            print(f\"{'  ' * depth}Right:\")\n",
    "            print_tree(node.right, depth + 1, feature_names)\n",
    "\n",
    "def predict(instance, node):\n",
    "    # Traverse the tree based on the features in the instance until a leaf node is reached.\n",
    "    while not node.is_leaf():\n",
    "        # Use the feature index directly as it's stored in the tree.\n",
    "        feature_index = node.feature\n",
    "        \n",
    "        # Traverse to the left or right child node based on the instance's feature value.\n",
    "        if instance[feature_index] == 0:\n",
    "            node = node.left\n",
    "        else:\n",
    "            node = node.right\n",
    "\n",
    "        # If the tree is not well-formed or the instance is missing features, return None or a default value.\n",
    "        if node is None:\n",
    "            return None\n",
    "\n",
    "    # Return the most common class label from the leaf node.\n",
    "    return np.argmax(node.value)\n",
    "\n",
    "# Define your instance as before\n",
    "instance = np.array([1, 0, 0, 1, 1, 1])  # Example instance\n",
    "\n",
    "# Call the predict function with this instance and the trained tree root node.\n",
    "prediction = predict(instance, root_node_c45)  # Make sure you use the correct root node for the C4.5 tree.\n",
    "\n",
    "# Output the prediction\n",
    "print(f\"The predicted class for the instance is: {prediction}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
